import sys
import os

# è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„è¿™ä¸ªæ–‡ä»¶ (.py æˆ– .ipynb) çš„ç»å¯¹è·¯å¾„
if '__file__' in locals():
    current_dir = os.path.dirname(os.path.abspath(__file__))
else:
    # é€‚é… Jupyter Notebook ç¯å¢ƒ
    current_dir = os.getcwd()

# å°†å½“å‰ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„ï¼Œç¡®ä¿èƒ½ import åŒçº§çš„ bronze, silver, pyspark_common ç­‰
if current_dir not in sys.path:
    sys.path.append(current_dir)

# å¦‚æœä½ çš„ great_expectations_common.py åœ¨ include/pyspark ä¸‹
# ä½†ä½ æƒ³å¼•ç”¨ include/ ä¸‹çš„å…¶ä»–ä¸œè¥¿ï¼Œå¯ä»¥å†å¾€ä¸Šä¸€å±‚
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)
import great_expectations as gx
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, LongType
import traceback
import os
import json
import threading
import gc
import os

# ==========================================
# 1. åŸºç¡€é…ç½® (é€‚é…æœ¬åœ°/ClickHouse)
# ==========================================
# åŠ¨æ€è·å–å½“å‰ Notebook æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()

# ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä½ gx_configs
# å‡è®¾ gx_configs å°±åœ¨ä½ å½“å‰ Notebook çš„åŒçº§ç›®å½•ä¸‹
BASE_PATH = os.path.join(current_dir, "gx_configs/expectations/")
DB_NAME = "news"
QUARANTINE_TABLE = "data_quality_quarantine"



_SHARED_GX_CONTEXT = None
_CACHED_SUITES_JSON = {}
gx_lock = threading.RLock() 

# å‡è®¾ DBsetup å·²ç»åœ¨ä¸»ç¨‹åºä¸­åˆå§‹åŒ–
# global DBsetup

# ==========================================
# 2. é…ç½®é¢„åŠ è½½
# ==========================================
def preload_all_suites():
    global _CACHED_SUITES_JSON
    if not os.path.exists(BASE_PATH):
        print(f"âš ï¸ Warning: Path not found {BASE_PATH}")
        return
    files = [f for f in os.listdir(BASE_PATH) if f.endswith(".json")]
    for f in files:
        suite_name = f.replace(".json", "")
        try:
            with open(os.path.join(BASE_PATH, f), "r", encoding='utf-8') as file:
                suite_dict = json.load(file)
                # ç§»é™¤å¯èƒ½å¼•èµ·å†²çªçš„å…ƒæ•°æ®å­—æ®µ
                suite_dict.pop("name", None)
                suite_dict.pop("data_context_id", None)
                _CACHED_SUITES_JSON[suite_name] = suite_dict
            print(f"âœ… Preloaded Suite: {suite_name}")
        except Exception as e:
            print(f"âŒ Load error {f}: {e}")

preload_all_suites()

def get_gx_context():
    global _SHARED_GX_CONTEXT
    with gx_lock:
        if _SHARED_GX_CONTEXT is None:
            _SHARED_GX_CONTEXT = gx.get_context(mode="ephemeral")
        return _SHARED_GX_CONTEXT

def load_suite_simple(context, suite_name):
    try:
        return context.suites.get(name=suite_name)
    except Exception:
        if suite_name in _CACHED_SUITES_JSON:
            suite_data = _CACHED_SUITES_JSON[suite_name]
            new_suite = gx.ExpectationSuite(
                name=suite_name, 
                expectations=suite_data.get("expectations", [])
            )
            return context.suites.add(new_suite)
        else:
            raise FileNotFoundError(f"Suite {suite_name} not found in cache.")

# ==========================================
# 3. æ ¸å¿ƒå¤„ç†å‡½æ•°
# ==========================================
def validate_and_insert_process_batch(df, batch_id, table_name, db_setup_manager): 
    """
    db_setup_manager: ä¼ å…¥ä¹‹å‰çš„ DBSetupManager å®ä¾‹
    """
    spark_internal = df.sparkSession
    if df.limit(1).count() == 0: return

    temp_id_col = "_dq_batch_id"
    ds_name = f"ds_{table_name}_{batch_id}"
    val_def_name = f"val_{table_name}_{batch_id}"
    
    # 1. ç”Ÿæˆå”¯ä¸€ ID ç”¨äºå®šä½åè¡Œ
    df_with_id = df.withColumn(temp_id_col, F.monotonically_increasing_id()).persist()
    result = None 

    # --- é”å†…ï¼šGX éªŒè¯æµ ---
    with gx_lock:
        try:
            print(f"ğŸ”’ Batch {batch_id}: Processing {table_name}...", flush=True)
            context = get_gx_context()
            
            # æ¸…ç†æ—§å®šä¹‰
            for n in [val_def_name, ds_name]:
                try: 
                    context.validation_definitions.delete(n) if "val" in n else context.data_sources.delete(n)
                except: pass

            datasource = context.data_sources.add_spark(name=ds_name)
            asset = datasource.add_dataframe_asset(name=f"asset_{batch_id}")
            batch_def = asset.add_batch_definition_whole_dataframe(name="batch_def")
            suite = load_suite_simple(context, f"{table_name}_suite")
            
            val_definition = context.validation_definitions.add(
                gx.ValidationDefinition(name=val_def_name, data=batch_def, suite=suite)
            )

            print(f"ğŸš€ Batch {batch_id}: Running GX for {table_name}...", flush=True)
            result = val_definition.run(
                batch_parameters={"dataframe": df_with_id},
                result_format={"result_format": "COMPLETE", "unexpected_index_column_names": [temp_id_col]}
            )
            
            # æ¸…ç†èµ„æº
            context.validation_definitions.delete(val_def_name)
            context.data_sources.delete(ds_name)
        except Exception as e:
            print(f"âŒ Batch {batch_id} GX Error: {str(e)}", flush=True)
            # æŠ¥é”™åˆ™å…¨é‡ä¿åº•å†™å…¥
            db_setup_manager.manager.fast_insert(df_with_id.drop(temp_id_col), table_name)
            return 
        finally:
            gc.collect()

    # --- é”å¤–ï¼šåˆ†æµå†™å…¥ ClickHouse ---
    try:
        # æƒ…å†µ A: å…¨éƒ¨é€šè¿‡
        if result and result.success:
            print(f"âœ… Batch {batch_id}: {table_name} Passed.", flush=True)
            db_setup_manager.manager.fast_insert(df_with_id.drop(temp_id_col), table_name)
        
        # æƒ…å†µ B: æœ‰å¤±è´¥è¡Œ
        elif result:
            errors = []
            for r in result.results:
                if not r.success:
                    conf = r.expectation_config
                    col = conf.kwargs.get("column", "Table")
                    rule = conf.type
                    ids = r.result.get("unexpected_index_list")
                    if ids:
                        for row_id_dict in ids:
                            val = row_id_dict.get(temp_id_col)
                            if val is not None:
                                errors.append((val, f"[{col}] {rule}"))
            
            # B1: è¡¨çº§é”™è¯¯ï¼ˆæ²¡æœ‰ä»»ä½•å…·ä½“è¡Œ IDï¼‰
            if not errors: 
                print(f"ğŸš¨ Batch {batch_id}: {table_name} TABLE-LEVEL ERROR! Quarantining entire batch.", flush=True)
                bad_df = df_with_id.withColumn("violated_rules", F.lit("Table-level Error")) \
                    .withColumn("raw_data", F.to_json(F.struct([c for c in df.columns]))) \
                    .select(F.lit(table_name).alias("table_name"), 
                            F.lit(str(batch_id)).alias("gx_batch_id"),
                            "violated_rules", "raw_data")
                db_setup_manager.manager.fast_insert(bad_df, QUARANTINE_TABLE)
                return

            # B2: è¡Œçº§é”™è¯¯éš”ç¦»
            error_schema = StructType([StructField(temp_id_col, LongType(), True), StructField("violated_rule", StringType(), True)])
            error_info_df = spark_internal.createDataFrame(errors, schema=error_schema) \
                .groupBy(temp_id_col).agg(F.concat_ws("; ", F.collect_list("violated_rule")).alias("violated_rules"))

            bad_row_ids = [e[0] for e in errors]
            
            # éš”ç¦»è„æ•°æ®
            bad_df = df_with_id.filter(F.col(temp_id_col).isin(bad_row_ids)).join(error_info_df, on=temp_id_col, how="left") \
                .withColumn("raw_data", F.to_json(F.struct([c for c in df.columns]))) \
                .select(F.lit(table_name).alias("table_name"), 
                        F.lit(str(batch_id)).alias("gx_batch_id"),
                        "violated_rules", "raw_data")
            
            db_setup_manager.manager.fast_insert(bad_df, QUARANTINE_TABLE)
            
            # å†™å…¥å¥½æ•°æ®
            good_df = df_with_id.filter(~F.col(temp_id_col).isin(bad_row_ids)).drop(temp_id_col)
            if good_df.limit(1).count() > 0:
                db_setup_manager.manager.fast_insert(good_df, table_name)
            
            print(f"âš ï¸ Batch {batch_id}: {table_name} FAILED. Quarantined {len(set(bad_row_ids))} rows.", flush=True)

    except Exception as e:
        print(f"âŒ Batch {batch_id} Final Write Error: {str(e)}", flush=True)
        # ä¿åº•é€»è¾‘ï¼šå°è¯•å…¨é‡å†™å…¥ç›®æ ‡è¡¨
        db_setup_manager.manager.fast_insert(df_with_id.drop(temp_id_col), table_name)
    finally:
        if df_with_id.is_cached: df_with_id.unpersist()
        gc.collect()