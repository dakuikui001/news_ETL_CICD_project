import os
import sys
import requests
import clickhouse_connect
from pyspark.sql import SparkSession

class ClickHouseSparkManager:
    def __init__(self, db_host="host.docker.internal", db_user="default", db_password="123456", db_port="8123", db_name="news"):
        # 1. åŸºç¡€é…ç½® (ç»Ÿä¸€ä½¿ç”¨å°å†™ news)
        self.db_name = db_name
        self.db_host = db_host
        self.db_port = db_port
        self.db_user = db_user
        self.db_password = db_password
        
        # 2. é©±åŠ¨é…ç½® (ä¿ç•™ JDBC é©±åŠ¨ä»¥å¤‡ä¸æ—¶ä¹‹éœ€ï¼Œä½†ä¸å†ä¾èµ–å®ƒå†™å…¥)
        self.jar_name = "clickhouse-jdbc-0.6.4-all.jar"
        self.jar_url = f"https://github.com/ClickHouse/clickhouse-java/releases/download/v0.6.4/{self.jar_name}"
        
        # 3. åˆå§‹åŒ–åŸç”Ÿè¿æ¥å‚æ•° (ç”¨äº clickhouse-connect)
        self.conn_params = {
            "host": self.db_host,
            "port": int(self.db_port),
            "username": self.db_user,
            "password": self.db_password,
            "database": self.db_name,
            "connect_timeout": 30
        }
        
        self._client = None

    @property
    def client(self):
        """æ‡’åŠ è½½åŸç”Ÿå®¢æˆ·ç«¯ï¼Œç¡®ä¿è¿æ¥åœ¨éœ€è¦æ—¶æ‰å»ºç«‹"""
        if self._client is None:
            try:
                self._client = clickhouse_connect.get_client(**self.conn_params)
            except Exception as e:
                print(f"âš ï¸ ClickHouse åŸç”Ÿè¿æ¥å¤±è´¥: {e}")
        return self._client

    def _prepare_jdbc_driver(self):
        current_dir = os.path.dirname(os.path.abspath(__file__))
        jar_path = os.path.join(current_dir, self.jar_name)
        if not os.path.exists(jar_path):
            print(f"ğŸšš æ­£åœ¨ä¸‹è½½é©±åŠ¨...")
            response = requests.get(self.jar_url, stream=True)
            with open(jar_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
        return jar_path

    def create_session(self, app_name="NewsAnalysisProject"):
        os.environ['PYSPARK_PYTHON'] = sys.executable
        os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
        jar_path = self._prepare_jdbc_driver()
        
        return SparkSession.builder \
            .appName(app_name) \
            .config("spark.jars", jar_path) \
            .config("spark.sql.caseSensitive", "true") \
            .getOrCreate()

    def execute_ddl(self, sql_command):
        """ä½¿ç”¨åŸç”Ÿå®¢æˆ·ç«¯æ‰§è¡Œ DDL (æ¯” JDBC ç¨³å¾—å¤š)"""
        try:
            self.client.command(sql_command)
            return True
        except Exception as e:
            print(f"âŒ DDL æ‰§è¡Œå¤±è´¥: {e}")
            return False

    def fast_insert(self, df, table_name):
        params = self.conn_params
        # è·å– DataFrame çš„æ‰€æœ‰åˆ—åï¼Œç¡®ä¿åªæ’å…¥è¿™äº›åˆ—
        columns = df.columns 
        
        def batch_insert(partition):
            import clickhouse_connect
            local_client = clickhouse_connect.get_client(**params)
            # å°† Row è½¬ä¸º List è€Œä¸æ˜¯ Dictï¼Œè¿™æ ·é…åˆ column å†™å…¥æœ€ç¨³
            batch = [list(row) for row in partition]
            if batch:
                try:
                    # æ˜ç¡®æŒ‡å®šåˆ—åè¿›è¡Œæ’å…¥ï¼Œè·³è¿‡æœ‰é»˜è®¤å€¼çš„ load_time
                    local_client.insert(table_name, batch, column_names=columns)
                except Exception as e:
                    print(f"âŒ åˆ†åŒºå†™å…¥å¤±è´¥: {e}")
                finally:
                    local_client.close()

        print(f"ğŸš€ æ­£åœ¨é€šè¿‡ HTTP åè®®åˆ†å¸ƒå¼å†™å…¥åˆ° {table_name} (åˆ—: {columns})...")
        df.rdd.foreachPartition(batch_insert)
        print(f"âœ… å†™å…¥å®Œæˆ")